{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_GPT35_DEPLOYMENT_NAME = os.getenv(\"OPENAI_GPT35_DEPLOYMENT_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "             deployment_name=OPENAI_GPT35_DEPLOYMENT_NAME,\n",
    "             openai_api_version=\"2024-02-15-preview\",\n",
    "             temperature=0,\n",
    "             max_tokens=400\n",
    "             ):\n",
    "\n",
    "    llm = AzureChatOpenAI(deployment_name=deployment_name,\n",
    "                            model=model,\n",
    "                            openai_api_version=openai_api_version,\n",
    "                            azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "                            temperature=temperature,\n",
    "                            max_tokens=max_tokens\n",
    "                            )\n",
    "    return llm\n",
    "\n",
    "llm = init_llm()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retain conversation history** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Large Language Models (LLMs) are stateless. This means that they donâ€™t retain any information about the conversation history.\n",
    "Each transaction is independent of the previous one. Chatbots keep in memory the conversation history and use it to generate the next response. This is why they are able to generate more coherent responses.\n",
    "\n",
    "Previously we saw that a model fails to answer the question that requires context. We can solve this problem by retaining the conversation history. We can do this by using the LangChain ConversationBufferMemory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "\n",
    "            Current conversation:\n",
    "            {history}\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can override it and set it to \"AI Assistant\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(memory_key=\"history\", ai_prefix=\"AI Assistant\", human_prefix=\"Human\"),\n",
    ")\n",
    "res = conversation.invoke(input=\"Hello!\")\n",
    "display(Markdown(res[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, I'd be happy to tell you a joke! Here it is: Why don't scientists trust atoms? Because they make up everything!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = conversation.invoke(input=\"I need you to tell me a joke!\")\n",
    "display(Markdown(res[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! Here's a joke about Portugal: Why did the Portuguese football team bring string to the game? Because they wanted to tie the score!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = conversation.invoke(input=\"Now do one about Portugal!\")\n",
    "display(Markdown(res[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here's a joke about Lisbon: Why did the baker in Lisbon always win awards? Because he always kneaded the dough-lightful pastries!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = conversation.invoke(input=\"I want it to be about Lisbon.\")\n",
    "display(Markdown(res[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello!'),\n",
       " AIMessage(content='Hello! How can I assist you today?'),\n",
       " HumanMessage(content='I need you to tell me a joke!'),\n",
       " AIMessage(content=\"Sure, I'd be happy to tell you a joke! Here it is: Why don't scientists trust atoms? Because they make up everything!\"),\n",
       " HumanMessage(content='Now do one about Portugal!'),\n",
       " AIMessage(content=\"Of course! Here's a joke about Portugal: Why did the Portuguese football team bring string to the game? Because they wanted to tie the score!\"),\n",
       " HumanMessage(content='I want it to be about Lisbon.'),\n",
       " AIMessage(content=\"Sure, here's a joke about Lisbon: Why did the baker in Lisbon always win awards? Because he always kneaded the dough-lightful pastries!\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.chat_memory.messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More conversation memory types.\n",
    "\n",
    "Keeping full conversation history could be expensive and we can hit the Azure API limits. LangChain provides different types of conversation memory that can be used to keep the conversation history and mitigate the limits issues.\n",
    "\n",
    "**1. ConversationBufferWindowMemory** - keeps the last N messages in the conversation history.\n",
    "\n",
    "**2. ConversationTokenBufferMemory** - keeps the last N tokens in the conversation history.\n",
    "\n",
    "**3. ConversationSummaryBufferMemory** - keeps summary of the conversation over time.\n",
    "\n",
    "Additional Memory Type includes:\n",
    "\n",
    "**4. Vector data memory** - stires the text in a vector DB and retrieves most semantically similar text.\n",
    "\n",
    "**5. Entity memories** - use LLM which rememebers details about specific entities. \n",
    "\n",
    "Note: You can use multiple memories at the same time. \n",
    "\n",
    "Finally you can store the conevrsation history in a conventional database like SQL or NoSQL. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: hello\\nHuman: What's your name\\nAI: My name is John\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"What's your name\"},\n",
    "                    {\"output\": \"My name is John\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
